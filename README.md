# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-17 06:36:35 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)

**Authors**: Fatemeh Lotfi, Fatemeh Afghah  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 13.5  
**Type**: new  
**ArXiv ID**: 2512.13715v1  

#### Abstract
The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resourc...

---

### 2. [Fast and Accurate Causal Parallel Decoding using Jacobi Forcing](https://arxiv.org/abs/2512.14681)

**Authors**: Lanxiang Hu, Siqi Kou, Yichao Fu, Samyam Rajbhandari, Tajana Rosing, Yuxiong He, Zhijie Deng, Hao Zhang  
**Category**: cs.CL  
**Published**: 2025-12-17  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.14681v1  

#### Abstract
Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques ada...

---

### 3. [A Unified Sparse Attention via Multi-Granularity Compression](https://arxiv.org/abs/2512.14082)

**Authors**: Siran Liu, Zane Cao, Yongchao He  
**Category**: cs.CL  
**Published**: 2025-12-17  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.14082v1  

#### Abstract
Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottlenec...

---

### 4. [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)

**Authors**: Yuhan Tang, Kangxin Cui, Jung Ho Park, Yibo Zhao, Xuan Jiang, Haoze He, Dingyi Zhuang, Shenhao Wang, Jiangbo Yu, Haris Koutsopoulos, Jinhua Zhao  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2512.13727v1  

#### Abstract
Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch r...

---

### 5. [Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification](https://arxiv.org/abs/2512.14491)

**Authors**: Cheng-Han Lu, Pei-Hsuan Tsai  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.14491v1  

#### Abstract
Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robus...

---

### 6. [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)

**Authors**: Gangesh Pathak, Prasanna Kumar  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.13714v1  

#### Abstract
LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior...

---

### 7. [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)

**Authors**: Alireza Olama, Andreas Lundell, Izzat El Hajj, Johan Lilius, Jerker Bj\"orkqvist  
**Category**: cs.DC  
**Published**: 2025-12-17  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.14628v1  

#### Abstract
Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured s...

---

### 8. [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)

**Authors**: Kaiming Luo  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.13708v1  

#### Abstract
The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) ...

---

### 9. [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)

**Authors**: Junjie Ma, Jinlong Li  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.14069v1  

#### Abstract
Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking fle...

---

### 10. [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)

**Authors**: Yitong Luo, Ziang Chen, Hou Hei Lam, Jiayu zhan, Junqi Wang, Zhenliang Zhang, Xue Feng  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.13716v1  

#### Abstract
Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical...

---

### 11. [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)

**Authors**: Siyuan Zhu, Chengdong Xu, Kaiqiang Ke, Chao Yu  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.14465v1  

#### Abstract
In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such...

---

### 12. [Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing](https://arxiv.org/abs/2512.14002)

**Authors**: Chuanchao Gao, Arvind Easwaran  
**Category**: cs.DC  
**Published**: 2025-12-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.14002v1  

#### Abstract
Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading ...

---

### 13. [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)

**Authors**: Shicheng Liu, Siyuan Xu, Wenjie Qiu, Hangfan Zhang, Minghui Zhu  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.13837v1  

#### Abstract
A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) ...

---

### 14. [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)

**Authors**: Jia Hu, Junqi Li, Weimeng Lin, Peng Jia, Yuxiong Ji, Jintao Lai  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.14417v1  

#### Abstract
Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high ...

---

### 15. [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)

**Authors**: Subramanyam Sahoo, Jared Junkin  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.13821v1  

#### Abstract
Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through se...

---

### 16. [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)

**Authors**: Rae Chipera, Jenny Du, Irene Tsapara  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.14675v1  

#### Abstract
Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activa...

---

### 17. [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)

**Authors**: Yankai Jiang, Yujie Zhang, Peng Zhang, Yichen Li, Jintai Chen, Xiaoming Shi, Shihui Zhen  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.14157v1  

#### Abstract
Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuch...

---

### 18. [VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse](https://arxiv.org/abs/2512.14531)

**Authors**: Ying Nie, Kai Han, Hongguang Li, Hang Zhou, Tianyu Guo, Enhua Wu, Xinghao Chen, Yunhe Wang  
**Category**: cs.CL  
**Published**: 2025-12-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.14531v1  

#### Abstract
The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting t...

---

### 19. [TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines](https://arxiv.org/abs/2512.14645)

**Authors**: David Schulmeister, Valentin Hartmann, Lars Klein, Robert West  
**Category**: cs.CL  
**Published**: 2025-12-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.14645v1  

#### Abstract
Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to ...

---

### 20. [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)

**Authors**: Da Zhang, Bingyu Li, Zhiyuan Zhao, Feiping Nie, Junyu Gao, Xuelong Li  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.14078v1  

#### Abstract
Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing ...

---

### 21. [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)

**Authors**: Chunjin Jian, Xinhua Zhu  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.14100v1  

#### Abstract
Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Po...

---

### 22. [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)

**Authors**: Zheng Xing, Junting Chen  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.13701v1  

#### Abstract
Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio ma...

---

### 23. [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)

**Authors**: Steve Nwaiwu, Nipat Jongsawat, Anucha Tungkasthan  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.13725v1  

#### Abstract
Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming sta...

---

### 24. [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)

**Authors**: Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, Xiaoyang Wang, Zhanping Zhong, Yun Zhu, Dahua Lin, Conghui He, Lijun Wu  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.14051v1  

#### Abstract
The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provena...

---

### 25. [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)

**Authors**: Mayank Singh, Vikas Yadav, Shiva Krishna Reddy Malay, Shravan Nayak, Sai Rajeswar, Sathwik Tejaswi Madhusudhan, Eduardo Blanco  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.14079v1  

#### Abstract
Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple...

---

### 26. [Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents](https://arxiv.org/abs/2512.14142)

**Authors**: Hongqiu Ni, Jiabao Zhang, Guopeng Li, Zilong Wang, Ruiqi Wu, Chi Zhang, Haisheng Tan  
**Category**: cs.CL  
**Published**: 2025-12-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.14142v1  

#### Abstract
Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing ...

---

### 27. [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)

**Authors**: Aadya Goel, Mayuri Sridhar  
**Category**: cs.LG  
**Published**: 2025-12-17  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.13711v1  

#### Abstract
Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for docu...

---

### 28. [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)

**Authors**: TK Lee  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.13762v1  

#### Abstract
Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon inter...

---

### 29. [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)

**Authors**: Can Jin, Hongwu Peng, Mingcan Xiang, Qixin Zhang, Xiangchi Yuan, Amit Hasan, Ohiremen Dibua, Yifan Gong, Yan Kang, Dimitris N. Metaxas  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.13996v1  

#### Abstract
Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flex...

---

### 30. [TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation](https://arxiv.org/abs/2512.14358)

**Authors**: Qizhi Wang  
**Category**: cs.AI  
**Published**: 2025-12-17  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.14358v1  

#### Abstract
Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper pres...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- framework, System, Generation, Linear, LLM, RL, RLHF, Reinforcement learning, Reinforcement Learning, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Parallelism, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
