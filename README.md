# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-23 05:52:47 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training](https://arxiv.org/abs/2512.18127)

**Authors**: Yi Yang, Ziyu Lin, Liesheng Wei  
**Category**: cs.DC  
**Published**: 2025-12-23  
**Score**: 15.0  
**Type**: new  
**ArXiv ID**: 2512.18127v1  

#### Abstract
Large-scale deep learning models impose substantial communication overh ead in distributed training, particularly in bandwidth-constrained or heterogeneous clo ud-edge environments. Conventional synchronous or fixed-compression techniques o ften struggle to balance communication cost, convergence st...

---

### 2. [Towards Efficient Agents: A Co-Design of Inference Architecture and System](https://arxiv.org/abs/2512.18337)

**Authors**: Weizhe Lin, Hui-Ling Zhen, Shuai Yang, Xian Wang, Renxi Liu, Hanting Chen, Wangze Zhang, Chuansai Zhou, Yiming Li, Chen Chen, Xing Li, Zhiyuan Yang, Xiaosong Li, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan, Yunhe Wang  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 14.5  
**Type**: new  
**ArXiv ID**: 2512.18337v1  

#### Abstract
The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the...

---

### 3. [Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation](https://arxiv.org/abs/2512.19361)

**Authors**: Isshaan Singh, Divyansh Chawla, Anshu Garg, Shivin Mangal, Pallavi Gupta, Khushi Agarwal, Nimrat Singh Khalsa, Nandan Patel  
**Category**: cs.LG  
**Published**: 2025-12-23  
**Score**: 13.0  
**Type**: new  
**ArXiv ID**: 2512.19361v1  

#### Abstract
The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision makin...

---

### 4. [Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap](https://arxiv.org/abs/2512.18126)

**Authors**: Zijun Wang, Yijiahao Qi, Hanqiu Chen, Zishen Wan, Gongjin Sun, Dongyang Li, Shuyi Pei, Cong Hao  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.18126v1  

#### Abstract
Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs wi...

---

### 5. [TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale](https://arxiv.org/abs/2512.18194)

**Authors**: Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim  
**Category**: cs.DC  
**Published**: 2025-12-23  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.18194v1  

#### Abstract
Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and ...

---

### 6. [NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic](https://arxiv.org/abs/2512.18453)

**Authors**: Jayant Lohia  
**Category**: cs.LG  
**Published**: 2025-12-23  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.18453v1  

#### Abstract
Winograd convolution is the standard algorithm for efficient inference, reducing arithmetic complexity by 2.25x for 3x3 kernels. However, it faces a critical barrier in the modern era of low precision computing: numerical instability. As tiles scale to maximize efficiency (e.g., F(6,3), F(8,3)), the...

---

### 7. [CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs](https://arxiv.org/abs/2512.19305)

**Authors**: Javier Vela-Tambo, Jorge Gracia, Fernando Dominguez-Castro  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.19305v1  

#### Abstract
Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM use...

---

### 8. [Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application](https://arxiv.org/abs/2512.19299)

**Authors**: Haoyu Jiang, Fanjie Zeng, Boan Qu, Xiaojie Lin, Wei Zhong  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.19299v1  

#### Abstract
In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awarenes...

---

### 9. [ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning](https://arxiv.org/abs/2512.18571)

**Authors**: Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu, Wei Li, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang, Zhengyou Zhang  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.18571v1  

#### Abstract
Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., "fetch the tool" in a cluttered room), current agents often fail to balance the high cost of physical ex...

---

### 10. [L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling](https://arxiv.org/abs/2512.19179)

**Authors**: Yitao Yuan (Peking University, ScitiX AI), Chenqi Zhao (Peking University), Bohan Zhao (ScitiX AI), Zane Cao (ScitiX AI), Yongchao He (ScitiX AI), Wenfei Wu (Peking University)  
**Category**: cs.DC  
**Published**: 2025-12-23  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.19179v1  

#### Abstract
Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the...

---

### 11. [MoE-TransMov: A Transformer-based Model for Next POI Prediction in Familiar & Unfamiliar Movements](https://arxiv.org/abs/2512.17985)

**Authors**: Ruichen Tan, Jiawei Xue, Kota Tsubouchi, Takahiro Yabe, Satish V. Ukkusuri  
**Category**: cs.LG  
**Published**: 2025-12-23  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.17985v1  

#### Abstract
Accurate prediction of the next point of interest (POI) within human mobility trajectories is essential for location-based services, as it enables more timely and personalized recommendations. In particular, with the rise of these approaches, studies have shown that users exhibit different POI choic...

---

### 12. [Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks](https://arxiv.org/abs/2512.18094)

**Authors**: Boxuan Wang, Zhuoyun Li, Xiaowei Huang, Yi Dong  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.18094v1  

#### Abstract
Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or...

---

### 13. [NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework](https://arxiv.org/abs/2512.18189)

**Authors**: Zihao Deng, Yijia Li, Renrui Zhang, Peijun Ye  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.18189v1  

#### Abstract
Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descript...

---

### 14. [SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.19317)

**Authors**: A. A. Gde Yogi Pramana, Jason Ray, Anthony Jaya, Michael Wijaya  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.19317v1  

#### Abstract
Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalizat...

---

### 15. [Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight](https://arxiv.org/abs/2512.19691)

**Authors**: Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.19691v1  

#### Abstract
Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-...

---

### 16. [GeoSense-AI: Fast Location Inference from Crisis Microblogs](https://arxiv.org/abs/2512.18225)

**Authors**: Deepit Sapru  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.18225v1  

#### Abstract
This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambig...

---

### 17. [AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards](https://arxiv.org/abs/2512.19126)

**Authors**: Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao, Guojun Yin, Wei Lin, Ran He  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.19126v1  

#### Abstract
While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and o...

---

### 18. [Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives](https://arxiv.org/abs/2512.19342)

**Authors**: Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman  
**Category**: cs.DC  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.19342v1  

#### Abstract
Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge...

---

### 19. [The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference](https://arxiv.org/abs/2512.19643)

**Authors**: Rajyasri Roy, Dibyajyoti Nayak, Somdatta Goswami  
**Category**: cs.LG  
**Published**: 2025-12-23  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.19643v1  

#### Abstract
Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across para...

---

### 20. [Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection](https://arxiv.org/abs/2512.18956)

**Authors**: Yizhi Wang, Linan Yue, Min-Ling Zhang  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.18956v1  

#### Abstract
Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarc...

---

### 21. [LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.18329)

**Authors**: Guo Chen, Junjie Huang, Huaijin Xie, Fei Sun, Tao Jia  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.18329v1  

#### Abstract
Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence a...

---

### 22. [From Word to World: Can Large Language Models be Implicit Text-based World Models?](https://arxiv.org/abs/2512.18832)

**Authors**: Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, Mengdi Wang  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.18832v1  

#### Abstract
Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whethe...

---

### 23. [Fast Online Digital Twinning on FPGA for Mission Critical Applications](https://arxiv.org/abs/2512.17942)

**Authors**: Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta  
**Category**: cs.DC  
**Published**: 2025-12-23  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.17942v1  

#### Abstract
Digital twinning enables real-time simulation and predictive modeling by maintaining a continuously updated virtual representation of a physical system. In mission-critical applications, such as mid-air collision avoidance, these models must operate online with extremely low latency to ensure safety...

---

### 24. [Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation](https://arxiv.org/abs/2512.18957)

**Authors**: Debamita Ghosh, George K. Atia, Yue Wang  
**Category**: cs.LG  
**Published**: 2025-12-23  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.18957v1  

#### Abstract
The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncerta...

---

### 25. [CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning](https://arxiv.org/abs/2512.18857)

**Authors**: Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.18857v1  

#### Abstract
Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, ...

---

### 26. [ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management](https://arxiv.org/abs/2512.19001)

**Authors**: Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.19001v1  

#### Abstract
As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR...

---

### 27. [Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning](https://arxiv.org/abs/2512.19081)

**Authors**: Yanzhi Zhang, Yitong Duan, Zhaoxi Zhang, Jiyan He, Shuxin Zheng  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.19081v1  

#### Abstract
Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic po...

---

### 28. [FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning](https://arxiv.org/abs/2512.19107)

**Authors**: Zhe Yang, Xiaoshuang Sheng, Zhengnan Zhang, Jidong Wu, Zexing Wang, Xin He, Shenghua Xu, Guanjing Xiong  
**Category**: cs.AI  
**Published**: 2025-12-23  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.19107v1  

#### Abstract
Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational cos...

---

### 29. [CodeSimpleQA: Scaling Factuality in Code Large Language Models](https://arxiv.org/abs/2512.19424)

**Authors**: Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang, Aishan Liu, Ge Zhang, Zili Wang, Zhoujun Li, Xianglong Liu, Weifeng Lv  
**Category**: cs.CL  
**Published**: 2025-12-23  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.19424v1  

#### Abstract
Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, ...

---

### 30. [LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform](https://arxiv.org/abs/2512.18266)

**Authors**: Lizhi Ma, Yi-Xiang Hu, Yuke Wang, Yifang Zhao, Yihui Ren, Jian-Xiang Liao, Feng Wu, Xiang-Yang Li  
**Category**: cs.LG  
**Published**: 2025-12-23  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.18266v1  

#### Abstract
With the rapid advancements in big data technologies, the Databricks platform has become a cornerstone for enterprises and research institutions, offering high computational efficiency and a robust ecosystem. However, managing the escalating operational costs associated with job execution remains a ...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- framework, System, Generation, Linear, LLM, RL, RLHF, Reinforcement learning, Reinforcement Learning, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Parallelism, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
