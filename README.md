# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-30 05:51:27 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)

**Authors**: Panlong Wu, Yifei Zhong, Danyang Chen, Ting Wang, Fangxin Wang  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 15.0  
**Type**: new  
**ArXiv ID**: 2512.22925v1  

#### Abstract
Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochasti...

---

### 2. [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)

**Authors**: Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 14.5  
**Type**: new  
**ArXiv ID**: 2512.22234v1  

#### Abstract
Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from compu...

---

### 3. [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)

**Authors**: Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 13.0  
**Type**: new  
**ArXiv ID**: 2512.22168v1  

#### Abstract
Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip...

---

### 4. [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)

**Authors**: Aiwei Liu, Minghua He, Shaoxun Zeng, Sijun Zhang, Linhao Zhang, Chuhan Wu, Wei Jia, Yuan Liu, Xiao Zhou, Jie Zhou  
**Category**: cs.CL  
**Published**: 2025-12-30  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.22737v1  

#### Abstract
Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they of...

---

### 5. [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)

**Authors**: Jithin VG, Ditto PS  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.22125v1  

#### Abstract
The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardw...

---

### 6. [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)

**Authors**: Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 12.5  
**Type**: new  
**ArXiv ID**: 2512.23236v1  

#### Abstract
Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEv...

---

### 7. [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)

**Authors**: Kun-Woo Shin (Seoul National University, Korea), Jay H. Park (Samsung Electronics, Korea), Moonwook Oh (Samsung Electronics, Korea), Yohan Jo (Seoul National University, Korea), Jaeyoung Do (Seoul National University, Korea), Sang-Won Lee (Seoul National University, Korea)  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.22195v1  

#### Abstract
We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing th...

---

### 8. [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)

**Authors**: Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.22178v1  

#### Abstract
The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demons...

---

### 9. [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)

**Authors**: Mona Moghadampanah, Adib Rezaei Shahmirzadi, Farhana Amin, Dimitrios S. Nikolopoulos  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.22695v1  

#### Abstract
Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly unders...

---

### 10. [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)

**Authors**: Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.22309v1  

#### Abstract
Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce ...

---

### 11. [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)

**Authors**: Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2512.22492v1  

#### Abstract
RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, ...

---

### 12. [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)

**Authors**: \"Unver \c{C}ift\c{c}i  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.22910v1  

#### Abstract
Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' ...

---

### 13. [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)

**Authors**: Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.23087v1  

#### Abstract
Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an ...

---

### 14. [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)

**Authors**: Jiangwen Dong, Jiayu Li, Wanyu Lin  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22137v1  

#### Abstract
Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mit...

---

### 15. [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)

**Authors**: Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22139v1  

#### Abstract
Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering ...

---

### 16. [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)

**Authors**: Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22420v1  

#### Abstract
Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Curr...

---

### 17. [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)

**Authors**: Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.22560v1  

#### Abstract
Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful,...

---

### 18. [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)

**Authors**: Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.22223v1  

#### Abstract
Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust....

---

### 19. [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)

**Authors**: Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.22245v1  

#### Abstract
As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or com...

---

### 20. [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)

**Authors**: S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.22897v1  

#### Abstract
Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches o...

---

### 21. [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)

**Authors**: Boyang Zhang, Xiaobing Chen, Songyang Zhang, Shuai Zhang, Xiangwei Zhou, Mingxuan Sun  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.23070v1  

#### Abstract
Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe ex...

---

### 22. [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)

**Authors**: Ziru Niu, Hai Dong, A. K. Qin, Tao Gu, Pengcheng Zhang  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.23200v1  

#### Abstract
Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and red...

---

### 23. [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)

**Authors**: Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.23367v1  

#### Abstract
Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extend...

---

### 24. [SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware](https://arxiv.org/abs/2512.22136)

**Authors**: Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee  
**Category**: cs.DC  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22136v1  

#### Abstract
Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that join...

---

### 25. [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)

**Authors**: Dhruv Tiwari  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22200v1  

#### Abstract
The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary...

---

### 26. [LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training](https://arxiv.org/abs/2512.22264)

**Authors**: Tzamn Melendez Carmona, Federico Marchesin, Marco P. Abrate, Peter Bienstman, Stefano Di Carlo, Alessandro Savino Senior  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22264v1  

#### Abstract
PNNs present promising opportunities for accelerating machine learning by leveraging the unique benefits of photonic circuits. However, current state of the art PNN simulation tools face significant scalability challenges when training large-scale PNNs, due to the computational demands of transfer m...

---

### 27. [Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing](https://arxiv.org/abs/2512.22296)

**Authors**: Reda Heddad, Lamiae Bouanane  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22296v1  

#### Abstract
The Mixture-of-Experts (MoE) architecture has emerged as a powerful paradigm for scaling deep learning models, yet it is fundamentally limited by challenges such as expert imbalance and the computational complexity of classical routing mechanisms. This paper investigates the potential of Quantum Mac...

---

### 28. [Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning](https://arxiv.org/abs/2512.22675)

**Authors**: Donghwa Kang, Shana Moothedath  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22675v1  

#### Abstract
Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task repr...

---

### 29. [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)

**Authors**: Gaurav Chaudhary, Laxmidhar Behera  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.22824v1  

#### Abstract
Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning process...

---

### 30. [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)

**Authors**: Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang  
**Category**: cs.LG  
**Published**: 2025-12-30  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.23461v1  

#### Abstract
Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For exa...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- framework, System, Generation, Linear, LLM, RL, RLHF, Reinforcement learning, Reinforcement Learning, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Parallelism, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
