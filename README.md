# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-05 11:02:57 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting](https://arxiv.org/abs/2512.04752)

**Authors**: Siqi Wang, Hailong Yang, Junjie Zhu, Xuezhu Wang, Yufan Xu, Depei Qian  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 13.0  
**Type**: new  
**ArXiv ID**: 2512.04752v1  

#### Abstract
Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe ...

---

### 2. [SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs](https://arxiv.org/abs/2512.04746)

**Authors**: Wenhua Cheng, Weiwei Zhang, Heng Guo, Haihao Shen  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.04746v1  

#### Abstract
Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-pr...

---

### 3. [Arbitrage: Efficient Reasoning via Advantage-Aware Speculation](https://arxiv.org/abs/2512.05033)

**Authors**: Monishwaran Maheswaran, Rishabh Tiwari, Yuezhou Hu, Kerem Dilmen, Coleman Hooper, Haocheng Xi, Nicholas Lee, Mehrdad Farajtabar, Michael W. Mahoney, Kurt Keutzer, Amir Gholami  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.05033v1  

#### Abstract
Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference ...

---

### 4. [Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning](https://arxiv.org/abs/2512.05105)

**Authors**: Purbesh Mitra, Sennur Ulukus  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.05105v1  

#### Abstract
Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programm...

---

### 5. [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)

**Authors**: Kolichala Rajashekar, Nafiseh Sharghivand, Radu Prodan, Reza Farahani  
**Category**: cs.DC  
**Published**: 2025-12-05  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.04088v1  

#### Abstract
Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inferenc...

---

### 6. [RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning](https://arxiv.org/abs/2512.04457)

**Authors**: Guoshenghui Zhao, Huawei Lin, Weijie Zhao  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.04457v1  

#### Abstract
Removing specific data influence from large language models (LLMs) remains challenging, as retraining is costly and existing approximate unlearning methods are often unstable. The challenge is exacerbated when the forget set is small or imbalanced. We introduce RapidUn, an influence-driven and param...

---

### 7. [Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems](https://arxiv.org/abs/2512.04476)

**Authors**: Zehao Fan, Zhenyu Liu, Yunzhen Liu, Yayue Hou, Hadjer Benmeziane, Kaoutar El Maghraoui, Liu Liu  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.04476v1  

#### Abstract
Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. W...

---

### 8. [BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training](https://arxiv.org/abs/2512.04189)

**Authors**: Luca Colombo, Fabrizio Pittorino, Daniele Zambon, Carlo Baldassi, Manuel Roveri, Cesare Alippi  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.04189v1  

#### Abstract
Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. Howeve...

---

### 9. [Decoding Large Language Diffusion Models with Foreseeing Movement](https://arxiv.org/abs/2512.04135)

**Authors**: Yichuan Mo, Quan Chen, Mingjie Li, Zeming Wei, Yisen Wang  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.04135v1  

#### Abstract
Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order o...

---

### 10. [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)

**Authors**: Wenlong Deng, Yushu Li, Boying Gong, Yi Ren, Christos Thrampoulidis, Xiaoxiao Li  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.04220v1  

#### Abstract
Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and ...

---

### 11. [EtCon: Edit-then-Consolidate for Reliable Knowledge Editing](https://arxiv.org/abs/2512.04753)

**Authors**: Ruilin Li, Yibin Wang, Wenhong Zhu, Chenglin Li, Jinghao Zhang, Chenliang Li, Junchi Yan, Jiaqi Wang  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.04753v1  

#### Abstract
Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing...

---

### 12. [SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs](https://arxiv.org/abs/2512.04868)

**Authors**: Hao Wang, Jialun Zhong, Changcheng Wang, Zhujun Nie, Zheng Li, Shunyu Yao, Yanzeng Li, Xinchi Li  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.04868v1  

#### Abstract
Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from...

---

### 13. [The Initialization Determines Whether In-Context Learning Is Gradient Descent](https://arxiv.org/abs/2512.04268)

**Authors**: Shifeng Xie, Rui Yuan, Simone Rossi, Thomas Hannagan  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.04268v1  

#### Abstract
In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions ...

---

### 14. [GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers](https://arxiv.org/abs/2512.04296)

**Authors**: Malyaban Bal, Abhronil Sengupta  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.04296v1  

#### Abstract
Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional ...

---

### 15. [QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction](https://arxiv.org/abs/2512.04596)

**Authors**: Guanchen Du, Jianlong Xu, Wei Wei  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.04596v1  

#### Abstract
Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit use...

---

### 16. [LangSAT: A Novel Framework Combining NLP and Reinforcement Learning for SAT Solving](https://arxiv.org/abs/2512.04374)

**Authors**: Muyu Pan, Matthew Walter, Dheeraj Kodakandla, Mahfuza Farooque  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.04374v1  

#### Abstract
Our work presents a novel reinforcement learning (RL) based framework to optimize heuristic selection within the conflict-driven clause learning (CDCL) process, improving the efficiency of Boolean satisfia- bility (SAT) solving. The proposed system, LangSAT, bridges the gap between natural language ...

---

### 17. [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)

**Authors**: Sushant Kumar Gupta, Anil Raghunath Iyer, Chang Yu, Neel Bagora, Olivier Pomerleau, Vivek Kumar, Prunthaban Kanthakumar  
**Category**: cs.DC  
**Published**: 2025-12-05  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.04096v1  

#### Abstract
Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data...

---

### 18. [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)

**Authors**: O. Tansel Baydas, Ozgur B. Akan  
**Category**: cs.DC  
**Published**: 2025-12-05  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.04984v1  

#### Abstract
The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic...

---

### 19. [Efficient Generative Transformer Operators For Million-Point PDEs](https://arxiv.org/abs/2512.04974)

**Authors**: Armand Kassa\"i Koupa\"i, Lise Le Boudec, Patrick Gallinari  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.04974v1  

#### Abstract
We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dy...

---

### 20. [David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?](https://arxiv.org/abs/2512.05073)

**Authors**: Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Animesh Basak Chowdhury, Sukanta Bhattacharjee, Chandan Karfa  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.5  
**Type**: new  
**ArXiv ID**: 2512.05073v1  

#### Abstract
Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated age...

---

### 21. [EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion](https://arxiv.org/abs/2512.04545)

**Authors**: Pengfei Cao, Zeao Ji, Daojian Zeng, Jun Zhao, Kang Liu  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.04545v1  

#### Abstract
Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch...

---

### 22. [AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees](https://arxiv.org/abs/2512.04550)

**Authors**: Yangning Li, Shaoshen Chen, Yinghui Li, Yankai Chen, Hai-Tao Zheng, Hui Wang, Wenhao Jiang, Philip S. Yu  
**Category**: cs.CL  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.04550v1  

#### Abstract
The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing ...

---

### 23. [RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection](https://arxiv.org/abs/2512.04333)

**Authors**: Shreyas Shende, Varsha Narayanan, Vishal Fenn, Yiran Huang, Dincer Goksuluk, Gaurav Choudhary, Melih Agraz, Mengjia Xu  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.04333v1  

#### Abstract
Early detection of cancer plays a key role in improving survival rates, but identifying reliable biomarkers from RNA-seq data is still a major challenge. The data are high-dimensional, and conventional statistical methods often fail to capture the complex relationships between genes. In this study, ...

---

### 24. [Score Matching for Estimating Finite Point Processes](https://arxiv.org/abs/2512.04617)

**Authors**: Haoqun Cao, Yixuan Zhang, Feng Zhou  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.04617v1  

#### Abstract
Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estim...

---

### 25. [Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty](https://arxiv.org/abs/2512.04918)

**Authors**: Kailiang Liu, Ying Chen, Ralf Bornd\"orfer, Thorsten Koch  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.04918v1  

#### Abstract
Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MA...

---

### 26. [TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103)

**Authors**: Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05103v1  

#### Abstract
Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from rec...

---

### 27. [The Universal Weight Subspace Hypothesis](https://arxiv.org/abs/2512.05117)

**Authors**: Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.05117v1  

#### Abstract
We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization...

---

### 28. [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)

**Authors**: Guang Yang, Tianpei Yang, Jingwen Qiao, Yanqing Wu, Jing Huo, Xingguo Chen, Yang Gao  
**Category**: cs.AI  
**Published**: 2025-12-05  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.03528v1  

#### Abstract
Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robu...

---

### 29. [Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection](https://arxiv.org/abs/2512.05069)

**Authors**: Mohammad Arif Rasyidi, Omar Alhussein, Sami Muhaidat, Ernesto Damiani  
**Category**: cs.LG  
**Published**: 2025-12-05  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.05069v1  

#### Abstract
Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that ite...

---

### 30. [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)

**Authors**: Yusen Wu, Xiaotie Deng  
**Category**: cs.AI  
**Published**: 2025-12-05  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.03607v1  

#### Abstract
This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modal...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- Linear Attention, LLM, RL, RLHF, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
