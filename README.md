# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2026-01-01 05:54:08 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [Training Report of TeleChat3-MoE](https://arxiv.org/abs/2512.24157)

**Authors**: Xinzhang Liu, Chao Wang, Zhihao Yang, Zhuo Jiang, Xuncheng Zhao, Haoran Wang, Lei Li, Dongdong He, Luobin Liu, Kaizhe Yuan, Han Gao, Zihan Wang, Yitong Yao, Sishi Xiong, Wenmin Deng, Haowei He, Kaidong Yu, Yu Zhao, Ruiyu Fang, Yuhao Jiang, Yingyan Li, Xiaohui Hu, Xi Yu, Jingqi Li, Yanwei Liu, Qingli Li, Xinyu Shi, Junhao Niu, Chengnuo Huang, Yao Xiao, Ruiwen Wang, Fengkai Li, Luwen Pu, Kaipeng Jia, Fubei Yao, Yuyao Huang, Xuewei He, Zhuoru Jiang, Ruiting Song, Rui Xue, Qiyi Xie, Jie Zhang, Zilu Huang, Zhaoxi Zhang, Zhilong Lu, Yanhan Zhang, Yin Zhang, Yanlei Xue, Zhu Yuan, Teng Su, Xin Jiang, Shuangyong Song, Yongxiang Li, Xuelong Li  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 19.0  
**Type**: new  
**ArXiv ID**: 2512.24157v1  

#### Abstract
TeleChat3-MoE is the latest series of TeleChat large language models, featuring a Mixture-of-Experts (MoE) architecture with parameter counts ranging from 105 billion to over one trillion,trained end-to-end on Ascend NPU cluster. This technical report mainly presents the underlying training infrastr...

---

### 2. [Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization](https://arxiv.org/abs/2512.24615)

**Authors**: Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 14.0  
**Type**: new  
**ArXiv ID**: 2512.24615v1  

#### Abstract
Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic ...

---

### 3. [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)

**Authors**: Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.24532v1  

#### Abstract
Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage ...

---

### 4. [MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling](https://arxiv.org/abs/2512.23824)

**Authors**: Mahdi Karami, Ali Behrouz, Peilin Zhong, Razvan Pascanu, Vahab Mirrokni  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.23824v1  

#### Abstract
State-space models (SSMs) have recently attention as an efficient alternative to computationally expensive attention-based models for sequence modeling. They rely on linear recurrences to integrate information over time, enabling fast inference, parallelizable training, and control over recurrence s...

---

### 5. [FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference](https://arxiv.org/abs/2512.24713)

**Authors**: Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 11.5  
**Type**: new  
**ArXiv ID**: 2512.24713v1  

#### Abstract
Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To ad...

---

### 6. [Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models](https://arxiv.org/abs/2512.24407)

**Authors**: Lars van der Laan, Aurelien Bibaut, Nathan Kallus  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2512.24407v1  

#### Abstract
Inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models explain sequential decision-making by recovering reward functions that rationalize observed behavior. Flexible IRL methods typically rely on machine learning but provide no guarantees for valid inference, while classical D...

---

### 7. [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/abs/2512.24873)

**Authors**: Weixun Wang, XiaoXiao Xu, Wanhe An, Fangwen Dai, Wei Gao, Yancheng He, Ju Huang, Qiang Ji, Hanqi Jin, Xiaoyang Li, Yang Li, Zhongwen Li, Shirong Lin, Jiashun Liu, Zenan Liu, Tao Luo, Dilxat Muhtar, Yuanbin Qu, Jiaqiang Shi, Qinghui Sun, Yingshui Tan, Hao Tang, Runze Wang, Yi Wang, Zhaoguo Wang, Yanan Wu, Shaopan Xiong, Binchen Xu, Xander Xu, Yuchi Xu, Qipeng Zhang, Xixia Zhang, Haizhou Zhao, Jie Zhao, Shuaibing Zhao, Baihui Zheng, Jianhui Zheng, Suhang Zheng, Yanni Zhu, Mengze Cai, Kerui Cao, Xitong Chen, Yue Dai, Lifan Du, Tao Feng, Tao He, Jin Hu, Yijie Hu, Ziyu Jiang, Cheng Li, Xiang Li, Jing Liang, Chonghuan Liu, ZhenDong Liu, Haodong Mi, Yanhu Mo, Junjia Ni, Shixin Pei, Jingyu Shen, XiaoShuai Song, Cecilia Wang, Chaofan Wang, Kangyu Wang, Pei Wang, Tao Wang, Wei Wang, Ke Xiao, Mingyu Xu, Tiange Xu, Nan Ya, Siran Yang, Jianan Ye, Yaxing Zang, Duo Zhang, Junbo Zhang, Boren Zheng, Wanxi Deng, Ling Pan, Lin Qu, Wenbo Su, Jiamang Wang, Wei Wang, Hu Wei, Minggang Wu, Cheng Yu, Bing Zhao, Zhicheng Zheng, Bo Zheng  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.24873v1  

#### Abstract
Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce...

---

### 8. [ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment](https://arxiv.org/abs/2512.24040)

**Authors**: Natchaya Temyingyong, Daman Jain, Neeraj Kumarsahu, Prabhat Kumar, Rachata Phondi, Wachiravit Modecrua, Krittanon Kaewtawee, Krittin Pachtrachai, Touchapon Kraisingkorn  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.24040v1  

#### Abstract
Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL)...

---

### 9. [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)

**Authors**: Mikaila J. Gossman, Avinash Maurya, Bogdan Nicolae, Jon C. Calhoun  
**Category**: cs.DC  
**Published**: 2026-01-01  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.24511v1  

#### Abstract
As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stab...

---

### 10. [SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing](https://arxiv.org/abs/2512.24008)

**Authors**: Gaurab Chhetri, Subasish Das, Tausif Islam Chowdhury  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.24008v1  

#### Abstract
Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in ...

---

### 11. [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)

**Authors**: Chunhui Wan, Xunan Dai, Zhuo Wang, Minglei Li, Yanpeng Wang, Yinan Mao, Yu Lan, Zhiwen Xiao  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.24077v1  

#### Abstract
The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address ...

---

### 12. [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)

**Authors**: Dong Qiu, Duo Xu, Limengxi Yue  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.24609v1  

#### Abstract
Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observabl...

---

### 13. [CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards](https://arxiv.org/abs/2512.23971)

**Authors**: Zhiming Lin, Kai Zhao, Sophie Zhang, Peilai Yu, Canran Xiao  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.23971v1  

#### Abstract
Large-scale Chinese spelling correction (CSC) remains critical for real-world text processing, yet existing LLMs and supervised methods lack robustness to novel errors and rely on costly annotations. We introduce CEC-Zero, a zero-supervision reinforcement learning framework that addresses this by en...

---

### 14. [Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding](https://arxiv.org/abs/2512.23858)

**Authors**: Yue Guan, Changming Yu, Shihan Fang, Weiming Hu, Zaifeng Pan, Zheng Wang, Zihan Liu, Yangjie Zhou, Yufei Ding, Minyi Guo, Jingwen Leng  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 9.0  
**Type**: new  
**ArXiv ID**: 2512.23858v1  

#### Abstract
Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions. We present Yggdrasil, a co-designed system that enables latency-o...

---

### 15. [Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning](https://arxiv.org/abs/2512.24265)

**Authors**: Ziqing Fan, Yuqiao Xian, Yan Sun, Li Shen  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.24265v1  

#### Abstract
A fine-grained data recipe is crucial for pre-training large language models, as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in ...

---

### 16. [Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline](https://arxiv.org/abs/2512.24933)

**Authors**: Minjun Zhao, Xinyu Zhang, Shuai Zhang, Deyang Li, Ruifeng Shi  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.24933v1  

#### Abstract
Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter...

---

### 17. [HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering](https://arxiv.org/abs/2512.24562)

**Authors**: Chaodong Tong, Qi Zhang, Jiayang Gao, Lei Jiang, Yanbing Liu, Nannan Sun  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.24562v1  

#### Abstract
Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods ...

---

### 18. [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)

**Authors**: Yongmin Zhang, Pengyu Huang, Mingyi Dong, Jing Yao  
**Category**: cs.DC  
**Published**: 2026-01-01  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.23952v1  

#### Abstract
Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optim...

---

### 19. [Mobility-Assisted Decentralized Federated Learning: Convergence Analysis and A Data-Driven Approach](https://arxiv.org/abs/2512.24694)

**Authors**: Reza Jahani, Md Farhamdur Reza, Richeng Jin, Huaiyu Dai  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.24694v1  

#### Abstract
Decentralized Federated Learning (DFL) has emerged as a privacy-preserving machine learning paradigm that enables collaborative training among users without relying on a central server. However, its performance often degrades significantly due to limited connectivity and data heterogeneity. As we mo...

---

### 20. [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)

**Authors**: Wei Wang, Nengneng Yu, Sixian Xiong, Zaoxing Liu  
**Category**: cs.DC  
**Published**: 2026-01-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.25059v1  

#### Abstract
Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training...

---

### 21. [Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data](https://arxiv.org/abs/2512.23761)

**Authors**: Esha Saha, Hao Wang  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.23761v1  

#### Abstract
Advances in data acquisition and computational methods have accelerated the use of differential equation based modelling for complex systems. Such systems are often described by coupled (or more) variables, yet governing equation is typically available for one variable, while the remaining variable ...

---

### 22. [Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks](https://arxiv.org/abs/2512.23905)

**Authors**: Peter Farag  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.23905v1  

#### Abstract
Dense linear layers are a dominant source of computational and parametric cost in modern machine learning models, despite their quadratic complexity and often being misaligned with the compositional structure of learned representations. We introduce Stagewise Pairwise Mixers (SPM), a structured line...

---

### 23. [A Scalable Framework for logP Prediction: From Terabyte-Scale Data Integration to Interpretable Ensemble Modeling](https://arxiv.org/abs/2512.24643)

**Authors**: Malikussaid, Septian Caesar Floresko, Ade Romadhony, Isman Kurniawan, Warih Maharani, Hilal Hudan Nuha  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.24643v1  

#### Abstract
This study presents a large-scale predictive modeling framework for logP prediction using 426850 bioactive compounds rigorously curated from the intersection of three authoritative chemical databases: PubChem, ChEMBL, and eMolecules. We developed a novel computational infrastructure to address the d...

---

### 24. [ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning](https://arxiv.org/abs/2512.25023)

**Authors**: Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke H\"ullermeier  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.25023v1  

#### Abstract
Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalizat...

---

### 25. [CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution](https://arxiv.org/abs/2512.23880)

**Authors**: Xu Huang, Junwu Chen, Yuxing Fei, Zhuohan Li, Philippe Schwaller, Gerbrand Ceder  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.23880v1  

#### Abstract
Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from "LLM + too...

---

### 26. [Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings](https://arxiv.org/abs/2512.25055)

**Authors**: Tianzhi He, Farrokh Jazizadeh  
**Category**: cs.AI  
**Published**: 2026-01-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.25055v1  

#### Abstract
This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three ...

---

### 27. [HY-MT1.5 Technical Report](https://arxiv.org/abs/2512.24092)

**Authors**: Mao Zheng, Zheng Li, Tao Chen, Mingyang Song, Di Wang  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.24092v1  

#### Abstract
In this report, we introduce our latest translation models, HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of machine translation models developed through a holistic training framework tailored for high-performance translation. Our methodology orchestrates a multi-stage pipeline that integrates general...

---

### 28. [Activation Steering for Masked Diffusion Language Models](https://arxiv.org/abs/2512.24143)

**Authors**: Adi Shnaidman, Erin Feiglin, Osher Yaari, Efrat Mentel, Amit Levi, Raz Lapid  
**Category**: cs.CL  
**Published**: 2026-01-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.24143v1  

#### Abstract
Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steerin...

---

### 29. [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)

**Authors**: Mingyi Li, Xiao Zhang, Ruisheng Zheng, Hongjian Shi, Yuan Yuan, Xiuzhen Cheng, Dongxiao Yu  
**Category**: cs.DC  
**Published**: 2026-01-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.24667v1  

#### Abstract
With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resour...

---

### 30. [Micro-Macro Tensor Neural Surrogates for Uncertainty Quantification in Collisional Plasma](https://arxiv.org/abs/2512.24205)

**Authors**: Wei Chen, Giacomo Dimarco, Lorenzo Pareschi  
**Category**: cs.LG  
**Published**: 2026-01-01  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.24205v1  

#### Abstract
Plasma kinetic equations exhibit pronounced sensitivity to microscopic perturbations in model parameters and data, making reliable and efficient uncertainty quantification (UQ) essential for predictive simulations. However, the cost of uncertainty sampling, the high-dimensional phase space, and mult...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- framework, System, Generation, Linear, LLM, RL, RLHF, Reinforcement learning, Reinforcement Learning, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Parallelism, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
