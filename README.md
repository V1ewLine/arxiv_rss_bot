# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-19 05:51:43 UTC
- **Total Papers Found**: 30
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines](https://arxiv.org/abs/2512.15946)

**Authors**: Dimitrios Danopoulos, Enrico Lupi, Chang Sun, Sebastian Dittmeier, Michael Kagan, Vladimir Loncar, Maurizio Pierini  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 16.0  
**Type**: new  
**ArXiv ID**: 2512.15946v1  

#### Abstract
Efficient AI inference on AMD's Versal AI Engine (AIE) is challenging due to tightly coupled VLIW execution, explicit datapaths, and local memory management. Prior work focused on first-generation AIE kernel optimizations, without tackling full neural network execution across the 2D array. In this w...

---

### 2. [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)

**Authors**: En-Ming Huang, Li-Shang Lin, Chun-Yi Lee  
**Category**: cs.DC  
**Published**: 2025-12-19  
**Score**: 13.5  
**Type**: new  
**ArXiv ID**: 2512.16473v1  

#### Abstract
Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets,...

---

### 3. [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)

**Authors**: Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen  
**Category**: cs.DC  
**Published**: 2025-12-19  
**Score**: 12.0  
**Type**: new  
**ArXiv ID**: 2512.16134v1  

#### Abstract
The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectur...

---

### 4. [LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding](https://arxiv.org/abs/2512.16229)

**Authors**: Chenkai Xu, Yijie Jin, Jiajun Li, Yi Tu, Guoping Long, Dandan Tu, Tianqi Hou, Junchi Yan, Zhijie Deng  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 11.0  
**Type**: new  
**ArXiv ID**: 2512.16229v1  

#### Abstract
Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the deg...

---

### 5. [TS-DP: Reinforcement Speculative Decoding For Temporal Adaptive Diffusion Policy Acceleration](https://arxiv.org/abs/2512.15773)

**Authors**: Ye Li, Jiahe Feng, Yuan Meng, Kangye Ji, Chen Tang, Xinwan Wen, Shutao Xia, Zhi Wang, Wenwu Zhu  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.15773v1  

#### Abstract
Diffusion Policy (DP) excels in embodied control but suffers from high inference latency and computational cost due to multiple iterative denoising steps. The temporal complexity of embodied tasks demands a dynamic and adaptable computation mode. Static and lossy acceleration methods, such as quanti...

---

### 6. [CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity](https://arxiv.org/abs/2512.16282)

**Authors**: Jinhao Zhang, Yunquan Zhang, Daning Chen  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.16282v1  

#### Abstract
Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quanti...

---

### 7. [Meta-RL Induces Exploration in Language Agents](https://arxiv.org/abs/2512.16848)

**Authors**: Yulun Jiang, Liangze Jiang, Damien Teney, Michael Moor, Maria Brbic  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 9.5  
**Type**: new  
**ArXiv ID**: 2512.16848v1  

#### Abstract
Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-e...

---

### 8. [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)

**Authors**: Jinwu Hu, Dongjin Yang, Langyu Bian, Zhiquan Wen, Yufeng Wang, Yaofo Chen, Bin Xiao, Yuanqing Li, Mingkui Tan  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.15089v1  

#### Abstract
Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of var...

---

### 9. [Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis](https://arxiv.org/abs/2512.15295)

**Authors**: Toshihide Ubukata, Enhong Mu, Takuto Yamauchi, Mingyue Zhang, Jialong Li, Kenji Tei  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.15295v1  

#### Abstract
Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or ...

---

### 10. [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)

**Authors**: Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan LU  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.15662v1  

#### Abstract
Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking ...

---

### 11. [BRAID: Bounded Reasoning for Autonomous Inference and Decisions](https://arxiv.org/abs/2512.15959)

**Authors**: Arma\u{g}an Amcalar, Eyup Cinar  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.15959v1  

#### Abstract
Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the Advance...

---

### 12. [Bridging Data and Physics: A Graph Neural Network-Based Hybrid Twin Framework](https://arxiv.org/abs/2512.15767)

**Authors**: M. Gorpinich (Valeo, PIMM Lab. ENSAM Institute of Technology), B. Moya (PIMM Lab. ENSAM Institute of Technology), S. Rodriguez (PIMM Lab. ENSAM Institute of Technology), F. Meraghni (PIMM Lab. ENSAM Institute of Technology), Y. Jaafra (Valeo), A. Briot (Valeo), M. Henner (Valeo), R. Leon (Valeo), F. Chinesta (PIMM Lab. ENSAM Institute of Technology, CNRS@CREATE LTD. Singapore)  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.15767v1  

#### Abstract
Simulating complex unsteady physical phenomena relies on detailed mathematical models, simulated for instance by using the Finite Element Method (FEM). However, these models often exhibit discrepancies from the reality due to unmodeled effects or simplifying assumptions. We refer to this gap as the ...

---

### 13. [Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game](https://arxiv.org/abs/2512.16626)

**Authors**: Barna P\'asztor, Thomas Kleine Buening, Andreas Krause  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 8.5  
**Type**: new  
**ArXiv ID**: 2512.16626v1  

#### Abstract
We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This...

---

### 14. [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)

**Authors**: Harsh Vardhan Bansal  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.16843v1  

#### Abstract
Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autor...

---

### 15. [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)

**Authors**: Eric Simon, Renato B. Hoffmann, Lucas Alf, Dalvan Griebler  
**Category**: cs.DC  
**Published**: 2025-12-19  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.16038v1  

#### Abstract
This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming...

---

### 16. [A Unified Generative-Predictive Framework for Deterministic Inverse Design](https://arxiv.org/abs/2512.15746)

**Authors**: Reza T. Batley, Sourav Saha  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.15746v1  

#### Abstract
Inverse design of heterogeneous material microstructures is a fundamentally ill-posed and famously computationally expensive problem. This is exacerbated by the high-dimensional design spaces associated with finely resolved images, multimodal input property streams, and a highly nonlinear forward ph...

---

### 17. [TENG++: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets under General Boundary Conditions](https://arxiv.org/abs/2512.15771)

**Authors**: Xinjie He, Chenggong Zhang  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 8.0  
**Type**: new  
**ArXiv ID**: 2512.15771v1  

#### Abstract
Partial Differential Equations (PDEs) are central to modeling complex systems across physical, biological, and engineering domains, yet traditional numerical methods often struggle with high-dimensional or complex problems. Physics-Informed Neural Networks (PINNs) have emerged as an efficient altern...

---

### 18. [LADY: Linear Attention for Autonomous Driving Efficiency without Transformers](https://arxiv.org/abs/2512.15038)

**Authors**: Jihao Huang, Xi Xia, Zhiyuan Li, Tianle Liu, Jingke Wang, Junbo Chen, Tengju Ye  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.15038v2  

#### Abstract
End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on res...

---

### 19. [Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants](https://arxiv.org/abs/2512.15712)

**Authors**: Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.15712v1  

#### Abstract
Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how inte...

---

### 20. [MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation](https://arxiv.org/abs/2512.16145)

**Authors**: Pengyu Wang, Shuchang Ye, Usman Naseem, Jinman Kim  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.16145v1  

#### Abstract
Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are tr...

---

### 21. [Twin Restricted Kernel Machines for Multiview Classification](https://arxiv.org/abs/2512.15757)

**Authors**: A. Quadir, M. Sajid, Mushir Akhtar, M. Tanveer  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.15757v1  

#### Abstract
Multi-view learning (MVL) is an emerging field in machine learning that focuses on improving generalization performance by leveraging complementary information from multiple perspectives or views. Various multi-view support vector machine (MvSVM) approaches have been developed, demonstrating signifi...

---

### 22. [Techno-economic optimization of a heat-pipe microreactor, part I: theory and cost optimization](https://arxiv.org/abs/2512.16032)

**Authors**: Paul Seurin, Dean Price, Luis Nunez  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.16032v1  

#### Abstract
Microreactors, particularly heat-pipe microreactors (HPMRs), are compact, transportable, self-regulated power systems well-suited for access-challenged remote areas where costly fossil fuels dominate. However, they suffer from diseconomies of scale, and their financial viability remains unconvincing...

---

### 23. [NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning](https://arxiv.org/abs/2512.16408)

**Authors**: Ruifeng Xu, Liang He  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.16408v1  

#### Abstract
Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying m...

---

### 24. [Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning](https://arxiv.org/abs/2512.16911)

**Authors**: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine  
**Category**: cs.LG  
**Published**: 2025-12-19  
**Score**: 7.5  
**Type**: new  
**ArXiv ID**: 2512.16911v1  

#### Abstract
Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical ...

---

### 25. [CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications](https://arxiv.org/abs/2512.15231)

**Authors**: Zhengchao Chen, Haoran Wang, Jing Yao, Pedram Ghamisi, Jun Zhou, Peter M. Atkinson, Bing Zhang  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.15231v1  

#### Abstract
The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--acro...

---

### 26. [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)

**Authors**: Jeongseok Kim, Kangjin Kim  
**Category**: cs.AI  
**Published**: 2025-12-19  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.15462v1  

#### Abstract
Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling probl...

---

### 27. [An Information-Theoretic Framework for Robust Large Language Model Editing](https://arxiv.org/abs/2512.16227)

**Authors**: Qizhou Chen, Chengyu Wang, Taolin Zhang, Xiaofeng He  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.16227v1  

#### Abstract
Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient ...

---

### 28. [Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains](https://arxiv.org/abs/2512.16401)

**Authors**: Darshil Chauhan, Adityasinh Solanki, Vansh Patel, Kanav Kapoor, Ritvik Jain, Aditya Bansal, Dhruv Kumar, Prateek Narang  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.16401v1  

#### Abstract
Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility i...

---

### 29. [JustRL: Scaling a 1.5B LLM with a Simple RL Recipe](https://arxiv.org/abs/2512.16649)

**Authors**: Bingxiang He, Zekai Qu, Zeyuan Liu, Yinghao Chen, Yuxin Zuo, Cheng Qian, Kaiyan Zhang, Weize Chen, Chaojun Xiao, Ganqu Cui, Ning Ding, Zhiyuan Liu  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.16649v1  

#### Abstract
Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \textbf{Is this complexity necessary?} We present \te...

---

### 30. [AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2512.16883)

**Authors**: Tzu-Han Lin, Wei-Lin Chen, Chen-An Li, Hung-yi Lee, Yun-Nung Chen, Yu Meng  
**Category**: cs.CL  
**Published**: 2025-12-19  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.16883v1  

#### Abstract
Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- framework, System, Generation, Linear, LLM, RL, RLHF, Reinforcement learning, Reinforcement Learning, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Parallelism, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
