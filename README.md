# arXiv Papers Bot ü§ñ

This repository automatically fetches and displays relevant papers from arXiv based on configured criteria.

## RSS Vercel Deployment [![An example of deployed RSS Server using vercel](https://img.shields.io/badge/Deployed-Example-blue)](https://arxiv.tachicoma.top/)

You can click this to deploy yours 

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/maydomine/arxiv_rss_bot)
## üìä Statistics

- **Last Updated**: 2025-12-26 05:51:31 UTC
- **Total Papers Found**: 27
- **Categories Monitored**: cs.AI, cs.CL, cs.DC, cs.LG

## üìö Recent Papers

### 1. [BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization](https://arxiv.org/abs/2512.20623)

**Authors**: Ravi Gupta, Shabista Haider  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 16.0  
**Type**: new  
**ArXiv ID**: 2512.20623v1  

#### Abstract
Smart home lighting systems consume 15-20% of residential energy but lack adaptive intelligence to optimize for user comfort and energy efficiency simultaneously. We present BitRL-Light, a novel framework combining 1-bit quantized Large Language Models (LLMs) with Deep Q-Network (DQN) reinforcement ...

---

### 2. [Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment](https://arxiv.org/abs/2512.20624)

**Authors**: Mazyar Taghavi, Javad Vahidi  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 13.0  
**Type**: new  
**ArXiv ID**: 2512.20624v1  

#### Abstract
This study introduces a quantum inspired framework for optimizing the exploration exploitation tradeoff in multiagent reinforcement learning, applied to UAVassisted 6G network deployment. We consider a cooperative scenario where ten intelligent UAVs autonomously coordinate to maximize signal coverag...

---

### 3. [The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents](https://arxiv.org/abs/2512.20884)

**Authors**: Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 10.5  
**Type**: new  
**ArXiv ID**: 2512.20884v1  

#### Abstract
Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frame...

---

### 4. [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)

**Authors**: Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 10.0  
**Type**: new  
**ArXiv ID**: 2512.20745v1  

#### Abstract
Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work,...

---

### 5. [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)

**Authors**: Leo Lu, Jonathan Zhang, Sean Chua, Spencer Kim, Kevin Zhu, Sean O'Brien, Vasu Sharma  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 7.0  
**Type**: new  
**ArXiv ID**: 2512.20647v1  

#### Abstract
Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In...

---

### 6. [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)

**Authors**: Zixun Luo, Yuhang Fan, Yufei Li, Youzhi Zhang, Hengyu Lin, Ziqi Wang  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20649v1  

#### Abstract
The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording ...

---

### 7. [AI-Driven Decision-Making System for Hiring Process](https://arxiv.org/abs/2512.20652)

**Authors**: Vira Filatova, Andrii Zelenchuk, Dmytro Filatov  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20652v1  

#### Abstract
Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document an...

---

### 8. [A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines](https://arxiv.org/abs/2512.20985)

**Authors**: Salman Jan, Hassan Ali Razzaqi, Ali Akarma, Mohammad Riyaz Belgaum  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 6.5  
**Type**: new  
**ArXiv ID**: 2512.20985v1  

#### Abstract
The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrit...

---

### 9. [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)

**Authors**: Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20831v1  

#### Abstract
Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning me...

---

### 10. [TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control](https://arxiv.org/abs/2512.20996)

**Authors**: Yuwei Du, Jun Zhang, Jie Feng, Zhicheng Liu, Jian Yuan, Yong Li  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 6.0  
**Type**: new  
**ArXiv ID**: 2512.20996v1  

#### Abstract
Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from...

---

### 11. [FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning](https://arxiv.org/abs/2512.20991)

**Authors**: Toqeer Ali Syed, Abdulaziz Alshahrani, Ali Ullah, Ali Akarma, Sohail Khan, Muhammad Nauman, Salman Jan  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.20991v1  

#### Abstract
The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household...

---

### 12. [Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation](https://arxiv.org/abs/2512.21066)

**Authors**: Tomoaki Yamaguchi, Yutong Zhou, Masahiro Ryo, Keisuke Katsura  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 5.0  
**Type**: new  
**ArXiv ID**: 2512.21066v1  

#### Abstract
Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for trans...

---

### 13. [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)

**Authors**: Yawei Liu  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 4.5  
**Type**: new  
**ArXiv ID**: 2512.20661v1  

#### Abstract
Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing model...

---

### 14. [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)

**Authors**: Deliang Wen, Ke Sun  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.20651v1  

#### Abstract
Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes ...

---

### 15. [Safety Alignment of LMs via Non-cooperative Games](https://arxiv.org/abs/2512.20806)

**Authors**: Anselm Paulus, Ilia Kulikov, Brandon Amos, R\'emi Munos, Ivan Evtimov, Kamalika Chaudhuri, Arman Zharmagambetov  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 4.0  
**Type**: new  
**ArXiv ID**: 2512.20806v1  

#### Abstract
Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: fram...

---

### 16. [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)

**Authors**: Esmail Gumaan  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2512.20650v1  

#### Abstract
The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) an...

---

### 17. [Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction](https://arxiv.org/abs/2512.20664)

**Authors**: Shinobu Miya  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 3.5  
**Type**: new  
**ArXiv ID**: 2512.20664v1  

#### Abstract
Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural c...

---

### 18. [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)

**Authors**: Jianbing Ma, Ao Feng, Zhenjie Gao, Xinyu Song, Li Su, Bin Chen, Wei Wang, Jiamin Wu  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 3.0  
**Type**: new  
**ArXiv ID**: 2512.20632v1  

#### Abstract
This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-tr...

---

### 19. [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)

**Authors**: Daan Di Scala, Sophie Lathouwers, Michael van Bekkum  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 3.0  
**Type**: new  
**ArXiv ID**: 2512.20671v1  

#### Abstract
Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, mak...

---

### 20. [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)

**Authors**: Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 3.0  
**Type**: new  
**ArXiv ID**: 2512.21110v1  

#### Abstract
Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumven...

---

### 21. [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)

**Authors**: Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 2.5  
**Type**: new  
**ArXiv ID**: 2512.20626v1  

#### Abstract
Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limi...

---

### 22. [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)

**Authors**: Aayam Bansal, Ishaan Gangwani  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 2.5  
**Type**: new  
**ArXiv ID**: 2512.20630v1  

#### Abstract
Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically sel...

---

### 23. [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)

**Authors**: Yiqing Ma, Jung-Hua Liu  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 2.5  
**Type**: new  
**ArXiv ID**: 2512.20662v1  

#### Abstract
Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or igno...

---

### 24. [MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs](https://arxiv.org/abs/2512.20845)

**Authors**: Onat Ozer, Grace Wu, Yuchen Wang, Daniel Dosti, Honghao Zhang, Vivi De La Rue  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 2.5  
**Type**: new  
**ArXiv ID**: 2512.20845v1  

#### Abstract
LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors ...

---

### 25. [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)

**Authors**: Enoch Hyunwook Kang  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 2.5  
**Type**: new  
**ArXiv ID**: 2512.21080v1  

#### Abstract
Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans wi...

---

### 26. [From artificial to organic: Rethinking the roots of intelligence for digital health](https://arxiv.org/abs/2512.20723)

**Authors**: Prajwal Ghimire, Keyoumars Ashkan  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 2.0  
**Type**: new  
**ArXiv ID**: 2512.20723v1  

#### Abstract
The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algo...

---

### 27. [RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic](https://arxiv.org/abs/2512.21220)

**Authors**: Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu  
**Category**: cs.AI  
**Published**: 2025-12-26  
**Score**: 1.0  
**Type**: new  
**ArXiv ID**: 2512.21220v1  

#### Abstract
Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a...

---

## üîß Configuration

This bot is configured to look for papers containing the following keywords:
- framework, System, Generation, Linear, LLM, RL, RLHF, Reinforcement learning, Reinforcement Learning, Inference, Training, Attention, Pipeline, MOE, Sparse, Quantization, Speculative, Efficient, Efficiency, Framework, Parallel, Parallelism, Distributed, Kernel, Decode, Decoding, Prefill, Throughput, Fast, Network, Hardware, Cluster, FP8, FP4, Optimization, Scalable, Communication

## üìÖ Schedule

The bot runs daily at 12:00 UTC via GitHub Actions to fetch the latest papers.

## üöÄ How to Use

1. **Fork this repository** to your GitHub account
2. **Customize the configuration** by editing `config.json`:
   - Add/remove arXiv categories (e.g., `cs.AI`, `cs.LG`, `cs.CL`)
   - Modify keywords to match your research interests
   - Adjust `max_papers` and `days_back` settings
3. **Enable GitHub Actions** in your repository settings
4. **The bot will automatically run daily** and update the README.md

## üìù Customization

### arXiv Categories
Common categories include:
- `cs.AI` - Artificial Intelligence
- `cs.LG` - Machine Learning
- `cs.CL` - Computation and Language
- `cs.CV` - Computer Vision
- `cs.NE` - Neural and Evolutionary Computing
- `stat.ML` - Machine Learning (Statistics)

### Keywords
Add keywords that match your research interests. The bot will search for these terms in paper titles and abstracts.

### Exclude Keywords
Add terms to exclude certain types of papers (e.g., "survey", "review", "tutorial").

## üîç Manual Trigger

You can manually trigger the bot by:
1. Going to the "Actions" tab in your repository
2. Selecting "arXiv Bot Daily Update"
3. Clicking "Run workflow"

---
*Generated automatically by arXiv Bot* 
